{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './twitter-datasets/'\n",
    "\n",
    "# Reading the data\n",
    "positive_path = os.path.join(data_folder,'train_pos.txt')\n",
    "negative_path = os.path.join(data_folder,'train_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_positive = [line.rstrip('\\n') for line in open(positive_path)]\n",
    "lines_negative = [line.rstrip('\\n') for line in open(negative_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt; the fame this way</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; what's the matter darling ? ?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they sending me home</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>had a great day catching up with friends yeste...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reflections on the psalms ( harvest book ) ( p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  sentiment\n",
       "0                           <user> the fame this way        1.0\n",
       "1               <user> what's the matter darling ? ?        0.0\n",
       "2                               they sending me home        1.0\n",
       "3  had a great day catching up with friends yeste...        0.0\n",
       "4  reflections on the psalms ( harvest book ) ( p...        0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataFrame from positive tweets and give them value 1 as a sentiment\n",
    "data_pos = pd.DataFrame({\"tweets\": lines_positive,\n",
    "                      \"sentiment\":np.ones(len(lines_positive))\n",
    "                      })\n",
    "\n",
    "# Create dataFrame from negative tweets and give them value 0 as a sentiment\n",
    "data_neg = pd.DataFrame({\"tweets\": lines_negative,\n",
    "                      \"sentiment\":np.zeros(len(lines_negative))\n",
    "                      })\n",
    "# Concat both of them\n",
    "data = pd.concat([data_pos,data_neg],axis=0).reset_index().drop(columns=['index'])\n",
    "\n",
    "# Shuffle everything so that we don't have all the positives in one cluster and all the negatives in another\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the fame this way</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what's the matter darling ? ?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they sending me home</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>had a great day catching up with friends yeste...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reflections on the psalms ( harvest book ) ( p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  sentiment\n",
       "0                                  the fame this way        1.0\n",
       "1                      what's the matter darling ? ?        0.0\n",
       "2                               they sending me home        1.0\n",
       "3  had a great day catching up with friends yeste...        0.0\n",
       "4  reflections on the psalms ( harvest book ) ( p...        0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove <anything> from tweets.\n",
    "data['tweets'].replace(regex=True,inplace=True,to_replace=r'<.*?>',value=r'')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['tweets'].tolist()\n",
    "y =  data['sentiment']\n",
    "# Split train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tokenizer\n",
    "### From words to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to numbers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103093"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'the': 2,\n",
       " 'to': 3,\n",
       " 'you': 4,\n",
       " 'a': 5,\n",
       " 'and': 6,\n",
       " 'my': 7,\n",
       " 'me': 8,\n",
       " 'of': 9,\n",
       " 'is': 10,\n",
       " 'for': 11,\n",
       " 'in': 12,\n",
       " 'it': 13,\n",
       " 'this': 14,\n",
       " 'so': 15,\n",
       " 'with': 16,\n",
       " 'on': 17,\n",
       " 'that': 18,\n",
       " 'be': 19,\n",
       " \"i'm\": 20,\n",
       " 'have': 21,\n",
       " 'but': 22,\n",
       " 'just': 23,\n",
       " 'rt': 24,\n",
       " 'love': 25,\n",
       " 'your': 26,\n",
       " 'all': 27,\n",
       " 'not': 28,\n",
       " 'was': 29,\n",
       " 'at': 30,\n",
       " 'are': 31,\n",
       " 'like': 32,\n",
       " 'get': 33,\n",
       " '3': 34,\n",
       " 'up': 35,\n",
       " 'frame': 36,\n",
       " 'lol': 37,\n",
       " 'good': 38,\n",
       " 'know': 39,\n",
       " 'u': 40,\n",
       " 'do': 41,\n",
       " 'now': 42,\n",
       " 'one': 43,\n",
       " 'when': 44,\n",
       " 'if': 45,\n",
       " 'we': 46,\n",
       " 'follow': 47,\n",
       " 'no': 48,\n",
       " 'can': 49,\n",
       " 'go': 50,\n",
       " 'what': 51,\n",
       " \"don't\": 52,\n",
       " 'x': 53,\n",
       " \"'\": 54,\n",
       " 'out': 55,\n",
       " 'will': 56,\n",
       " 'day': 57,\n",
       " '2': 58,\n",
       " 'please': 59,\n",
       " '1': 60,\n",
       " 'from': 61,\n",
       " 'see': 62,\n",
       " 'too': 63,\n",
       " 'want': 64,\n",
       " 'there': 65,\n",
       " 'back': 66,\n",
       " \"it's\": 67,\n",
       " 'today': 68,\n",
       " 'about': 69,\n",
       " 'really': 70,\n",
       " 'how': 71,\n",
       " 'got': 72,\n",
       " 'thanks': 73,\n",
       " 'time': 74,\n",
       " \"can't\": 75,\n",
       " 'its': 76,\n",
       " 'think': 77,\n",
       " 'im': 78,\n",
       " 'haha': 79,\n",
       " 'going': 80,\n",
       " 'he': 81,\n",
       " 'as': 82,\n",
       " 'miss': 83,\n",
       " 'by': 84,\n",
       " 'need': 85,\n",
       " 'an': 86,\n",
       " 'her': 87,\n",
       " 'they': 88,\n",
       " 'or': 89,\n",
       " 'well': 90,\n",
       " 'why': 91,\n",
       " 'our': 92,\n",
       " 'new': 93,\n",
       " 'much': 94,\n",
       " 'she': 95,\n",
       " 'more': 96,\n",
       " 'make': 97,\n",
       " 'paperback': 98,\n",
       " 'would': 99,\n",
       " 'then': 100,\n",
       " 'come': 101,\n",
       " 'some': 102,\n",
       " '4': 103,\n",
       " 'oh': 104,\n",
       " 'wish': 105,\n",
       " 'been': 106,\n",
       " 'only': 107,\n",
       " 'thank': 108,\n",
       " 'who': 109,\n",
       " \"i'll\": 110,\n",
       " 'd': 111,\n",
       " 'still': 112,\n",
       " 'had': 113,\n",
       " 'them': 114,\n",
       " 'happy': 115,\n",
       " 'tomorrow': 116,\n",
       " '5': 117,\n",
       " 'right': 118,\n",
       " 'best': 119,\n",
       " \"that's\": 120,\n",
       " 'here': 121,\n",
       " 'never': 122,\n",
       " 'work': 123,\n",
       " 'feel': 124,\n",
       " 'beautiful': 125,\n",
       " 'him': 126,\n",
       " 'hope': 127,\n",
       " 'am': 128,\n",
       " \"you're\": 129,\n",
       " 'has': 130,\n",
       " 'night': 131,\n",
       " 'black': 132,\n",
       " 'gonna': 133,\n",
       " 'home': 134,\n",
       " 'xx': 135,\n",
       " 'people': 136,\n",
       " 'wanna': 137,\n",
       " 'off': 138,\n",
       " 'yeah': 139,\n",
       " 'picture': 140,\n",
       " 'pack': 141,\n",
       " 'say': 142,\n",
       " '6': 143,\n",
       " 'great': 144,\n",
       " 'complete': 145,\n",
       " 'could': 146,\n",
       " 'last': 147,\n",
       " 'school': 148,\n",
       " 'life': 149,\n",
       " 'always': 150,\n",
       " 'way': 151,\n",
       " 'even': 152,\n",
       " 'should': 153,\n",
       " 'us': 154,\n",
       " 'better': 155,\n",
       " 'did': 156,\n",
       " 'sorry': 157,\n",
       " 's': 158,\n",
       " 'tweet': 159,\n",
       " '8': 160,\n",
       " 'his': 161,\n",
       " 'wait': 162,\n",
       " 'o': 163,\n",
       " 'take': 164,\n",
       " 'fans': 165,\n",
       " 'xxx': 166,\n",
       " 'girl': 167,\n",
       " 'over': 168,\n",
       " 'sad': 169,\n",
       " 'wide': 170,\n",
       " 'though': 171,\n",
       " 'tonight': 172,\n",
       " 'hate': 173,\n",
       " 'bad': 174,\n",
       " 'poster': 175,\n",
       " 'custom': 176,\n",
       " 'again': 177,\n",
       " 'next': 178,\n",
       " 'yes': 179,\n",
       " 'hey': 180,\n",
       " 'baby': 181,\n",
       " 'because': 182,\n",
       " 'twitter': 183,\n",
       " 'dont': 184,\n",
       " 'first': 185,\n",
       " 'sleep': 186,\n",
       " 'tell': 187,\n",
       " 'morning': 188,\n",
       " 'look': 189,\n",
       " 'where': 190,\n",
       " 'were': 191,\n",
       " 'man': 192,\n",
       " 'guys': 193,\n",
       " 'nice': 194,\n",
       " 'birthday': 195,\n",
       " 'ever': 196,\n",
       " \"i've\": 197,\n",
       " 'made': 198,\n",
       " 'world': 199,\n",
       " 'p': 200,\n",
       " 'someone': 201,\n",
       " 'little': 202,\n",
       " 'down': 203,\n",
       " 'live': 204,\n",
       " 'friends': 205,\n",
       " 'edition': 206,\n",
       " 'something': 207,\n",
       " 'watch': 208,\n",
       " 'being': 209,\n",
       " 'fun': 210,\n",
       " 'very': 211,\n",
       " 'after': 212,\n",
       " '10': 213,\n",
       " \"didn't\": 214,\n",
       " 'long': 215,\n",
       " 'omg': 216,\n",
       " 'phone': 217,\n",
       " 'getting': 218,\n",
       " 'give': 219,\n",
       " 'video': 220,\n",
       " 'soon': 221,\n",
       " 'okay': 222,\n",
       " 'let': 223,\n",
       " 'than': 224,\n",
       " 'w': 225,\n",
       " 'done': 226,\n",
       " 'week': 227,\n",
       " 'same': 228,\n",
       " 'year': 229,\n",
       " 'thing': 230,\n",
       " 'amazing': 231,\n",
       " '7': 232,\n",
       " 'shit': 233,\n",
       " 'ur': 234,\n",
       " 'bed': 235,\n",
       " 'talk': 236,\n",
       " 'cause': 237,\n",
       " 'said': 238,\n",
       " 'sure': 239,\n",
       " 'cute': 240,\n",
       " 'everyone': 241,\n",
       " 'god': 242,\n",
       " 'friend': 243,\n",
       " 'book': 244,\n",
       " 'any': 245,\n",
       " 'set': 246,\n",
       " 'these': 247,\n",
       " 'big': 248,\n",
       " 'pretty': 249,\n",
       " 'ok': 250,\n",
       " 'following': 251,\n",
       " 'keep': 252,\n",
       " 'game': 253,\n",
       " 'hahaha': 254,\n",
       " 'n': 255,\n",
       " 'days': 256,\n",
       " 'hardcover': 257,\n",
       " '12': 258,\n",
       " 'help': 259,\n",
       " 'find': 260,\n",
       " 'weekend': 261,\n",
       " 'cant': 262,\n",
       " 'two': 263,\n",
       " 'white': 264,\n",
       " 'other': 265,\n",
       " 'doing': 266,\n",
       " 'old': 267,\n",
       " 'cd': 268,\n",
       " 'already': 269,\n",
       " 'tho': 270,\n",
       " 'before': 271,\n",
       " 'text': 272,\n",
       " 'call': 273,\n",
       " 'high': 274,\n",
       " 'case': 275,\n",
       " 'thought': 276,\n",
       " 'things': 277,\n",
       " 'show': 278,\n",
       " 'their': 279,\n",
       " 'hard': 280,\n",
       " 'series': 281,\n",
       " 'everything': 282,\n",
       " 'having': 283,\n",
       " 'most': 284,\n",
       " 'cool': 285,\n",
       " 'dvd': 286,\n",
       " 't': 287,\n",
       " 'fuck': 288,\n",
       " 'mean': 289,\n",
       " 'makes': 290,\n",
       " 'girls': 291,\n",
       " \"won't\": 292,\n",
       " 'thats': 293,\n",
       " 'hi': 294,\n",
       " 'another': 295,\n",
       " 'stop': 296,\n",
       " 'b': 297,\n",
       " 'lmao': 298,\n",
       " 'every': 299,\n",
       " 'ill': 300,\n",
       " 'music': 301,\n",
       " '0': 302,\n",
       " 'actually': 303,\n",
       " 'hair': 304,\n",
       " 'song': 305,\n",
       " 'aww': 306,\n",
       " 'may': 307,\n",
       " \"what's\": 308,\n",
       " 'ya': 309,\n",
       " 'wood': 310,\n",
       " 'try': 311,\n",
       " 'must': 312,\n",
       " 'awesome': 313,\n",
       " \"he's\": 314,\n",
       " 'watching': 315,\n",
       " 'looking': 316,\n",
       " 'true': 317,\n",
       " 'm': 318,\n",
       " '20': 319,\n",
       " 'many': 320,\n",
       " 'away': 321,\n",
       " 'free': 322,\n",
       " 'person': 323,\n",
       " 'name': 324,\n",
       " 'into': 325,\n",
       " 'sooo': 326,\n",
       " 'via': 327,\n",
       " 'nothing': 328,\n",
       " 'play': 329,\n",
       " 'without': 330,\n",
       " 'anything': 331,\n",
       " 'wrong': 332,\n",
       " 'audio': 333,\n",
       " 'coming': 334,\n",
       " 'ass': 335,\n",
       " 'put': 336,\n",
       " 'those': 337,\n",
       " 'yet': 338,\n",
       " 'meet': 339,\n",
       " 'while': 340,\n",
       " 'real': 341,\n",
       " 'ready': 342,\n",
       " \"haven't\": 343,\n",
       " 'face': 344,\n",
       " 'since': 345,\n",
       " 'guess': 346,\n",
       " 'feeling': 347,\n",
       " 'heart': 348,\n",
       " 'maybe': 349,\n",
       " 'followers': 350,\n",
       " 'smile': 351,\n",
       " 'left': 352,\n",
       " 'c': 353,\n",
       " 'cry': 354,\n",
       " 'house': 355,\n",
       " 'went': 356,\n",
       " 'friday': 357,\n",
       " 'stay': 358,\n",
       " 'boys': 359,\n",
       " \"she's\": 360,\n",
       " 'class': 361,\n",
       " 'does': 362,\n",
       " 'use': 363,\n",
       " 'r': 364,\n",
       " '9': 365,\n",
       " 'hot': 366,\n",
       " 'finally': 367,\n",
       " 'trying': 368,\n",
       " 'also': 369,\n",
       " 'lot': 370,\n",
       " 'anymore': 371,\n",
       " 'saw': 372,\n",
       " 'damn': 373,\n",
       " 'believe': 374,\n",
       " 'kit': 375,\n",
       " 'awww': 376,\n",
       " 'missed': 377,\n",
       " 'tired': 378,\n",
       " 'start': 379,\n",
       " 'such': 380,\n",
       " \"doesn't\": 381,\n",
       " 'tweets': 382,\n",
       " 'excited': 383,\n",
       " 'sick': 384,\n",
       " 'early': 385,\n",
       " 'battery': 386,\n",
       " 'myself': 387,\n",
       " 'funny': 388,\n",
       " 'red': 389,\n",
       " 'wanted': 390,\n",
       " 'around': 391,\n",
       " 'might': 392,\n",
       " 'head': 393,\n",
       " 'family': 394,\n",
       " 'boy': 395,\n",
       " '11': 396,\n",
       " \"i'd\": 397,\n",
       " 'mom': 398,\n",
       " 'end': 399,\n",
       " 'memory': 400,\n",
       " 'through': 401,\n",
       " 'seeing': 402,\n",
       " 'years': 403,\n",
       " 'seen': 404,\n",
       " 'leave': 405,\n",
       " 'sweet': 406,\n",
       " 'remember': 407,\n",
       " 'eat': 408,\n",
       " 'gotta': 409,\n",
       " '16': 410,\n",
       " 'making': 411,\n",
       " 'gone': 412,\n",
       " 'inch': 413,\n",
       " 'brand': 414,\n",
       " 'guide': 415,\n",
       " 'mine': 416,\n",
       " 'food': 417,\n",
       " 'care': 418,\n",
       " 'moment': 419,\n",
       " 'brown': 420,\n",
       " 'ounce': 421,\n",
       " 'glad': 422,\n",
       " '100': 423,\n",
       " 'times': 424,\n",
       " '30': 425,\n",
       " 'kindle': 426,\n",
       " 'perfect': 427,\n",
       " 'blue': 428,\n",
       " 'working': 429,\n",
       " 'mind': 430,\n",
       " 'money': 431,\n",
       " 'read': 432,\n",
       " 'share': 433,\n",
       " 'top': 434,\n",
       " \"you'll\": 435,\n",
       " 'laptop': 436,\n",
       " 'car': 437,\n",
       " 'news': 438,\n",
       " 'looks': 439,\n",
       " 'whole': 440,\n",
       " 'summer': 441,\n",
       " 'job': 442,\n",
       " \"there's\": 443,\n",
       " 'l': 444,\n",
       " '25': 445,\n",
       " 'talking': 446,\n",
       " 'lost': 447,\n",
       " 'part': 448,\n",
       " 'full': 449,\n",
       " 'hear': 450,\n",
       " 'luck': 451,\n",
       " 'bit': 452,\n",
       " 'check': 453,\n",
       " 'told': 454,\n",
       " 'ask': 455,\n",
       " 'aw': 456,\n",
       " 'enough': 457,\n",
       " 'hours': 458,\n",
       " 'used': 459,\n",
       " 'both': 460,\n",
       " 'team': 461,\n",
       " 'anyone': 462,\n",
       " 'followed': 463,\n",
       " 'together': 464,\n",
       " 'win': 465,\n",
       " 'bitch': 466,\n",
       " 'till': 467,\n",
       " 'thinking': 468,\n",
       " 'until': 469,\n",
       " 'welcome': 470,\n",
       " 'cold': 471,\n",
       " 'liam': 472,\n",
       " 'fine': 473,\n",
       " 'waiting': 474,\n",
       " 'babe': 475,\n",
       " 'late': 476,\n",
       " 'ago': 477,\n",
       " 'later': 478,\n",
       " 'movie': 479,\n",
       " 'fan': 480,\n",
       " 'party': 481,\n",
       " 'each': 482,\n",
       " 'wow': 483,\n",
       " 'enjoy': 484,\n",
       " 'few': 485,\n",
       " 'guy': 486,\n",
       " 'lucky': 487,\n",
       " 'change': 488,\n",
       " 'buy': 489,\n",
       " 'story': 490,\n",
       " \"let's\": 491,\n",
       " 'which': 492,\n",
       " 'replacement': 493,\n",
       " 'dream': 494,\n",
       " 'own': 495,\n",
       " 'favorite': 496,\n",
       " 'wants': 497,\n",
       " 'bring': 498,\n",
       " 'hp': 499,\n",
       " 'super': 500,\n",
       " 'boyfriend': 501,\n",
       " 'yesterday': 502,\n",
       " '50': 503,\n",
       " 'weather': 504,\n",
       " 'bout': 505,\n",
       " 'brother': 506,\n",
       " 'special': 507,\n",
       " 'far': 508,\n",
       " \"isn't\": 509,\n",
       " 'fucking': 510,\n",
       " 'soo': 511,\n",
       " 'once': 512,\n",
       " 'power': 513,\n",
       " \"they're\": 514,\n",
       " 'shoutout': 515,\n",
       " 'rain': 516,\n",
       " 'hehe': 517,\n",
       " 'ugh': 518,\n",
       " 'stuff': 519,\n",
       " \"we're\": 520,\n",
       " '18': 521,\n",
       " 'justin': 522,\n",
       " 'mad': 523,\n",
       " 'single': 524,\n",
       " '15': 525,\n",
       " 'crying': 526,\n",
       " 'hell': 527,\n",
       " 'course': 528,\n",
       " 'found': 529,\n",
       " 'words': 530,\n",
       " 'crazy': 531,\n",
       " 'f': 532,\n",
       " 'tech': 533,\n",
       " 'else': 534,\n",
       " 'proud': 535,\n",
       " 'dear': 536,\n",
       " 'size': 537,\n",
       " 'saturday': 538,\n",
       " 'sucks': 539,\n",
       " 'least': 540,\n",
       " 'almost': 541,\n",
       " 'heard': 542,\n",
       " 'probably': 543,\n",
       " 'place': 544,\n",
       " 'goes': 545,\n",
       " 'everyday': 546,\n",
       " 'boo': 547,\n",
       " \"wasn't\": 548,\n",
       " 'e': 549,\n",
       " 'able': 550,\n",
       " 'sexy': 551,\n",
       " 'room': 552,\n",
       " 'alone': 553,\n",
       " 'ipod': 554,\n",
       " 'lovely': 555,\n",
       " 'health': 556,\n",
       " 'seriously': 557,\n",
       " 'ha': 558,\n",
       " 'playing': 559,\n",
       " 'green': 560,\n",
       " 'hurt': 561,\n",
       " 'oz': 562,\n",
       " 'camera': 563,\n",
       " 'sounds': 564,\n",
       " 'second': 565,\n",
       " 'weeks': 566,\n",
       " 'color': 567,\n",
       " 'small': 568,\n",
       " 'loves': 569,\n",
       " 'reply': 570,\n",
       " 'came': 571,\n",
       " 'hit': 572,\n",
       " 'forever': 573,\n",
       " 'poor': 574,\n",
       " 'yay': 575,\n",
       " 'body': 576,\n",
       " 'eyes': 577,\n",
       " 'yea': 578,\n",
       " 'idk': 579,\n",
       " 'gets': 580,\n",
       " 'dad': 581,\n",
       " 'cable': 582,\n",
       " 'date': 583,\n",
       " 'walk': 584,\n",
       " 'hello': 585,\n",
       " 'english': 586,\n",
       " 'send': 587,\n",
       " 'stupid': 588,\n",
       " 'sister': 589,\n",
       " 'didnt': 590,\n",
       " '2012': 591,\n",
       " 'saying': 592,\n",
       " 'notice': 593,\n",
       " 'beauty': 594,\n",
       " 'bored': 595,\n",
       " 'photo': 596,\n",
       " 'forget': 597,\n",
       " 'pain': 598,\n",
       " 'retweet': 599,\n",
       " 'number': 600,\n",
       " 'comes': 601,\n",
       " 'ima': 602,\n",
       " 'ah': 603,\n",
       " 'tickets': 604,\n",
       " 'kids': 605,\n",
       " 'digital': 606,\n",
       " 'american': 607,\n",
       " 'album': 608,\n",
       " 'pic': 609,\n",
       " 'half': 610,\n",
       " 'ram': 611,\n",
       " 'nooo': 612,\n",
       " 'voice': 613,\n",
       " 'lil': 614,\n",
       " 'run': 615,\n",
       " 'light': 616,\n",
       " 'woke': 617,\n",
       " 'missing': 618,\n",
       " 'na': 619,\n",
       " 'needs': 620,\n",
       " 'goodnight': 621,\n",
       " 'drive': 622,\n",
       " 'season': 623,\n",
       " 'easy': 624,\n",
       " 'hopefully': 625,\n",
       " 'hurts': 626,\n",
       " 'oomf': 627,\n",
       " 'dm': 628,\n",
       " 'city': 629,\n",
       " 'chance': 630,\n",
       " 'classic': 631,\n",
       " 'hand': 632,\n",
       " 'wrote': 633,\n",
       " 'y': 634,\n",
       " \"couldn't\": 635,\n",
       " 'took': 636,\n",
       " 'hour': 637,\n",
       " 'word': 638,\n",
       " 'concert': 639,\n",
       " 'cuz': 640,\n",
       " 'prom': 641,\n",
       " 'forward': 642,\n",
       " 'wake': 643,\n",
       " 'sometimes': 644,\n",
       " 'rest': 645,\n",
       " 'silver': 646,\n",
       " 'tv': 647,\n",
       " 'dog': 648,\n",
       " 'steel': 649,\n",
       " \"ain't\": 650,\n",
       " 'touch': 651,\n",
       " 'cat': 652,\n",
       " 'k': 653,\n",
       " 'dance': 654,\n",
       " 'called': 655,\n",
       " 'bag': 656,\n",
       " 'dress': 657,\n",
       " 'means': 658,\n",
       " 'count': 659,\n",
       " 'niall': 660,\n",
       " 'month': 661,\n",
       " 'bro': 662,\n",
       " 'side': 663,\n",
       " 'product': 664,\n",
       " 'three': 665,\n",
       " 'hologram': 666,\n",
       " 'kitchen': 667,\n",
       " 'worst': 668,\n",
       " 'jealous': 669,\n",
       " 'forgot': 670,\n",
       " 'dark': 671,\n",
       " 'th': 672,\n",
       " '24': 673,\n",
       " 'happen': 674,\n",
       " 'history': 675,\n",
       " 'taking': 676,\n",
       " 'business': 677,\n",
       " 'skin': 678,\n",
       " 'ones': 679,\n",
       " 'close': 680,\n",
       " \"we'll\": 681,\n",
       " 'h': 682,\n",
       " 'cover': 683,\n",
       " 'sound': 684,\n",
       " 'listen': 685,\n",
       " 'problem': 686,\n",
       " 'notebook': 687,\n",
       " 'yo': 688,\n",
       " 'reason': 689,\n",
       " 'g': 690,\n",
       " 'v': 691,\n",
       " 'hahah': 692,\n",
       " 'leaving': 693,\n",
       " 'laugh': 694,\n",
       " 'shirt': 695,\n",
       " 'box': 696,\n",
       " 'games': 697,\n",
       " 'outside': 698,\n",
       " 'ahhh': 699,\n",
       " 'either': 700,\n",
       " 'btw': 701,\n",
       " 'lunch': 702,\n",
       " 'move': 703,\n",
       " 'turn': 704,\n",
       " 'young': 705,\n",
       " 'trip': 706,\n",
       " 'tour': 707,\n",
       " 'wont': 708,\n",
       " 'electronics': 709,\n",
       " 'feet': 710,\n",
       " 'wit': 711,\n",
       " 'says': 712,\n",
       " 'open': 713,\n",
       " 'worry': 714,\n",
       " 'idea': 715,\n",
       " 'art': 716,\n",
       " 'point': 717,\n",
       " 'online': 718,\n",
       " 'harry': 719,\n",
       " 'card': 720,\n",
       " 'die': 721,\n",
       " 'style': 722,\n",
       " 'la': 723,\n",
       " 'screen': 724,\n",
       " 'cut': 725,\n",
       " 'pick': 726,\n",
       " 'study': 727,\n",
       " 'short': 728,\n",
       " 'mood': 729,\n",
       " 'listening': 730,\n",
       " 'met': 731,\n",
       " 'fall': 732,\n",
       " 'design': 733,\n",
       " 'college': 734,\n",
       " 'knew': 735,\n",
       " 'songs': 736,\n",
       " 'post': 737,\n",
       " 'fast': 738,\n",
       " 'plus': 739,\n",
       " 'test': 740,\n",
       " 'wear': 741,\n",
       " 'official': 742,\n",
       " \"y'all\": 743,\n",
       " 'finish': 744,\n",
       " 'busy': 745,\n",
       " 'pink': 746,\n",
       " 'pro': 747,\n",
       " 'smh': 748,\n",
       " 'scared': 749,\n",
       " 'alright': 750,\n",
       " 'wall': 751,\n",
       " 'nobody': 752,\n",
       " 'dinner': 753,\n",
       " 'facebook': 754,\n",
       " 'paper': 755,\n",
       " 'kind': 756,\n",
       " 'release': 757,\n",
       " 'toy': 758,\n",
       " 'yourself': 759,\n",
       " 'shout': 760,\n",
       " 'double': 761,\n",
       " 'ddr': 762,\n",
       " 'understand': 763,\n",
       " \"you've\": 764,\n",
       " 'uk': 765,\n",
       " 'quality': 766,\n",
       " 'sun': 767,\n",
       " 'gold': 768,\n",
       " 'reading': 769,\n",
       " 'ice': 770,\n",
       " 'manufactured': 771,\n",
       " 'ff': 772,\n",
       " 'started': 773,\n",
       " 'feels': 774,\n",
       " 'kinda': 775,\n",
       " 'em': 776,\n",
       " 'break': 777,\n",
       " 'collection': 778,\n",
       " 'louis': 779,\n",
       " 'support': 780,\n",
       " 'sports': 781,\n",
       " 'loved': 782,\n",
       " 'mum': 783,\n",
       " 'rock': 784,\n",
       " 'account': 785,\n",
       " 'inside': 786,\n",
       " 'books': 787,\n",
       " 'aint': 788,\n",
       " 'months': 789,\n",
       " 'direction': 790,\n",
       " 'bet': 791,\n",
       " 'leather': 792,\n",
       " 'bus': 793,\n",
       " 'gave': 794,\n",
       " 'coffee': 795,\n",
       " 'yours': 796,\n",
       " \"who's\": 797,\n",
       " 'happened': 798,\n",
       " 'self': 799,\n",
       " 'fat': 800,\n",
       " 'chocolate': 801,\n",
       " 'somebody': 802,\n",
       " 'yu': 803,\n",
       " 'ma': 804,\n",
       " 'water': 805,\n",
       " 'dead': 806,\n",
       " 'market': 807,\n",
       " 'line': 808,\n",
       " 'past': 809,\n",
       " 'sex': 810,\n",
       " 'totally': 811,\n",
       " '13': 812,\n",
       " 'join': 813,\n",
       " 'apple': 814,\n",
       " 'cream': 815,\n",
       " 'pair': 816,\n",
       " 'lady': 817,\n",
       " 'london': 818,\n",
       " 'death': 819,\n",
       " 'tried': 820,\n",
       " 'plan': 821,\n",
       " 'sunday': 822,\n",
       " 'save': 823,\n",
       " 'definitely': 824,\n",
       " 'round': 825,\n",
       " 'page': 826,\n",
       " '14': 827,\n",
       " 'deal': 828,\n",
       " 'smoking': 829,\n",
       " 'monday': 830,\n",
       " 'country': 831,\n",
       " 'eating': 832,\n",
       " 'system': 833,\n",
       " 'original': 834,\n",
       " 'future': 835,\n",
       " 'air': 836,\n",
       " 'anyway': 837,\n",
       " 'dude': 838,\n",
       " 'hug': 839,\n",
       " '1gb': 840,\n",
       " '000': 841,\n",
       " 'asleep': 842,\n",
       " 'learn': 843,\n",
       " 'iphone': 844,\n",
       " 'large': 845,\n",
       " 'upgrade': 846,\n",
       " 'april': 847,\n",
       " 'seems': 848,\n",
       " 'usb': 849,\n",
       " 'band': 850,\n",
       " 'ugly': 851,\n",
       " 'matter': 852,\n",
       " 'instead': 853,\n",
       " 'compatible': 854,\n",
       " 'pictures': 855,\n",
       " 'everybody': 856,\n",
       " 'anti': 857,\n",
       " 'zayn': 858,\n",
       " 'nd': 859,\n",
       " 'da': 860,\n",
       " 'kill': 861,\n",
       " 'catch': 862,\n",
       " 'lets': 863,\n",
       " 'hungry': 864,\n",
       " 'longer': 865,\n",
       " 'broke': 866,\n",
       " 'travel': 867,\n",
       " 'stories': 868,\n",
       " 'congrats': 869,\n",
       " '17': 870,\n",
       " 'clear': 871,\n",
       " 'hahahaha': 872,\n",
       " 'tweeting': 873,\n",
       " 'kiss': 874,\n",
       " 'star': 875,\n",
       " 'died': 876,\n",
       " 'practice': 877,\n",
       " 'lmfao': 878,\n",
       " 'kid': 879,\n",
       " 'minutes': 880,\n",
       " 'fact': 881,\n",
       " 'living': 882,\n",
       " 'performance': 883,\n",
       " 'takes': 884,\n",
       " 'twitcam': 885,\n",
       " 'knows': 886,\n",
       " 'yellow': 887,\n",
       " 'ahh': 888,\n",
       " 'mr': 889,\n",
       " 'imma': 890,\n",
       " 'personal': 891,\n",
       " 'drink': 892,\n",
       " \"wouldn't\": 893,\n",
       " 'mate': 894,\n",
       " 'answer': 895,\n",
       " 'blow': 896,\n",
       " 'front': 897,\n",
       " 'press': 898,\n",
       " 'goin': 899,\n",
       " 'exactly': 900,\n",
       " 'smoke': 901,\n",
       " 'exam': 902,\n",
       " 'finished': 903,\n",
       " 'wonder': 904,\n",
       " 'asked': 905,\n",
       " 'ive': 906,\n",
       " 'pm': 907,\n",
       " 'tea': 908,\n",
       " 'stand': 909,\n",
       " 'hang': 910,\n",
       " 'bestfriend': 911,\n",
       " 'visit': 912,\n",
       " 'under': 913,\n",
       " 'ohh': 914,\n",
       " 'road': 915,\n",
       " '75': 916,\n",
       " 'ppl': 917,\n",
       " 'pls': 918,\n",
       " 'different': 919,\n",
       " 'lots': 920,\n",
       " 'volume': 921,\n",
       " 'sold': 922,\n",
       " 'thursday': 923,\n",
       " 'running': 924,\n",
       " 'blog': 925,\n",
       " 'fit': 926,\n",
       " 'cell': 927,\n",
       " 'ball': 928,\n",
       " 'shower': 929,\n",
       " 'aha': 930,\n",
       " 'length': 931,\n",
       " 'yall': 932,\n",
       " 'loool': 933,\n",
       " \"women's\": 934,\n",
       " '1d': 935,\n",
       " 'couple': 936,\n",
       " 'gym': 937,\n",
       " 'town': 938,\n",
       " 'bought': 939,\n",
       " 'loving': 940,\n",
       " 'club': 941,\n",
       " 'parents': 942,\n",
       " 'agree': 943,\n",
       " 'includes': 944,\n",
       " 'version': 945,\n",
       " 'weird': 946,\n",
       " 'sleeping': 947,\n",
       " 'compaq': 948,\n",
       " 'write': 949,\n",
       " 'bye': 950,\n",
       " 'available': 951,\n",
       " 'between': 952,\n",
       " 'wonderful': 953,\n",
       " 'print': 954,\n",
       " 'link': 955,\n",
       " 'misc': 956,\n",
       " 'xd': 957,\n",
       " 'mobile': 958,\n",
       " 'tears': 959,\n",
       " 'beach': 960,\n",
       " 'dreams': 961,\n",
       " 'radio': 962,\n",
       " 'jus': 963,\n",
       " 'office': 964,\n",
       " 'low': 965,\n",
       " 'ring': 966,\n",
       " 'adapter': 967,\n",
       " 'computer': 968,\n",
       " 'thankyou': 969,\n",
       " 'french': 970,\n",
       " 'worth': 971,\n",
       " 'hold': 972,\n",
       " 'swear': 973,\n",
       " 'whats': 974,\n",
       " 'football': 975,\n",
       " 'plug': 976,\n",
       " 'imagine': 977,\n",
       " 'starting': 978,\n",
       " 'park': 979,\n",
       " 'homework': 980,\n",
       " 'women': 981,\n",
       " 'bday': 982,\n",
       " 'capacity': 983,\n",
       " 'natural': 984,\n",
       " 'youuu': 985,\n",
       " 'cried': 986,\n",
       " 'huge': 987,\n",
       " 'features': 988,\n",
       " 'library': 989,\n",
       " 'during': 990,\n",
       " 'type': 991,\n",
       " 'track': 992,\n",
       " 'trust': 993,\n",
       " 'wtf': 994,\n",
       " 'id': 995,\n",
       " 're': 996,\n",
       " 'giving': 997,\n",
       " 'beat': 998,\n",
       " 'fb': 999,\n",
       " 'whatever': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome ! thank you tv guide and yes - let's get dirty !   ->  [313, 108, 4, 647, 415, 6, 179, 491, 33, 1730]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0],' -> ',X_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['length'] = data['tweets'].str.split().apply(len)\n",
    "max_tokens = data['length'].max()\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of words used in a tweet is 64, we will pad every tweet to make it match this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_padded = pad_sequences(X_train_tokenized, maxlen=max_tokens,padding='pre', truncating='pre')\n",
    "X_test_padded = pad_sequences(X_test_tokenized, maxlen=max_tokens,padding='pre', truncating='pre')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_string(number_array, tokenizer):\n",
    "    \n",
    "    ''' \n",
    "    Transforms tokens to words\n",
    "    :param number_array: The numbers array to transform\n",
    "    :param tokenizer: The tokenizer used\n",
    "\n",
    "    :return: the original text\n",
    "    :rtype: String\n",
    "    '''\n",
    "        \n",
    "    indices = tokenizer.word_index\n",
    "    # Create a dict that mapes numbers to their respective words\n",
    "    inverse_map = dict(zip(indices.values(), indices.keys()))\n",
    "        \n",
    "    # Maps the numbers back to words.\n",
    "    words = []\n",
    "    for number in number_array:\n",
    "        if number != 0: # !=0 is to remove the padding\n",
    "            words.append(inverse_map[number])\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"awesome thank you tv guide and yes let's get dirty\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_to_string(X_train_tokenized[0],tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 62, 8)             824744    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 62, 16)            1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 62, 8)             600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 826,705\n",
      "Trainable params: 826,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 8\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      "  3136/144000 [..............................] - ETA: 17:45 - loss: 0.6916 - acc: 0.5293"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-789ea9e15367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train_padded, y_train,\n\u001b[0;32m----> 2\u001b[0;31m           validation_split=0.1, epochs=100, batch_size=64)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_padded, y_train,\n",
    "          validation_split=0.1, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
